Create a project to migrate data from an Oracle database to a MySQL tenant of OceanBase Database 
=====================================================================================================================

This topic describes how to use OceanBase Migration Service (OMS) to migrate data from an Oracle database to a MySQL tenant of OceanBase Database. 

Background information 
-------------------------------------------

You can create a data migration project in the OMS console to migrate the existing business data and incremental data from an Oracle database to a MySQL tenant of OceanBase Database through schema migration, full migration, and incremental data synchronization. 

The Oracle database supports the following modes: primary database only, standby database only, and primary/standby databases. The following table describes the data migration operations supported by each mode. 


|           Mode            |                                                                       Supported operations                                                                       |
|---------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Primary database only     | Schema migration, full migration, incremental migration, full verification, and reverse incremental migration                                                    |
| Standby database only     | Schema migration, full migration, incremental migration, and full verification                                                                                   |
| Primary/standby databases | Primary database: reverse incremental migration Standby database: schema migration, full migration, incremental migration, and full verification |



Prerequisites 
----------------------------------

* You have created a corresponding schema in the destination MySQL tenant of OceanBase Database. OMS allows you to migrate tables and columns. You must create a corresponding schema in the destination database before migration.

  

* You have enabled archivelog for the source Oracle instance and switched the logfile before OMS starts incremental data replication.

  

* You have installed the source Oracle instance, and LogMiner runs properly. 

  LogMiner enables you to obtain data from the archived logs of the Oracle database.
  

* You have created dedicated database users in the source Oracle database and the destination MySQL tenant of OceanBase Database for data migration and granted the corresponding privileges to the users. For more information, see [Create and authorize a database user](/en-US/5.user-guide/5.data-migration-1/4.preparation/1.create-and-authorize-a-database-user.md).

  

* You have made sure that the Oracle instance has enabled the database-level or table-level supplemental_log feature.

  

* If you enable supplemental_log of the primary key and unique key at the database level, when a large number of unnecessary logs are generated by tables that do not need to be synchronized, the pressure on LogMiner Reader to fetch logs and on the Oracle database increases. Therefore, you can enable only table-level supplemental_log of primary key and unique key for Oracle databases in the OMS console. However, if you configure the Set ETL Options to filter columns other than the primary key and unique key columns when you create a migration task, enable supplemental_log for the corresponding columns or all columns.

  




Limits 
---------------------------

* Incremental data migration is not supported for a table whose data in all columns is of the following three large object (LOB) types: BLOB, CLOB, and NCLOB.

  

* Oracle Database 11g, 12c, 18c, and 19c are supported, with the PDB and CDB modes supported in 12c and later versions.

  

* You cannot use DDL operations to migrate incremental data from an Oracle database to a MySQL tenant of OceanBase Database.

  

* OMS allows you to migrate data from the source Oracle instance that uses character sets including AL32UTF8, AL16UTF16, ZHS16GBK, or GB18030. If the character set of the Oracle database is different from that of the MySQL tenant of OceanBase Database, you can specify the field length mapping rule when you create the migration project.

  

* If you select a migration mode that supports incremental migration and reverse incremental migration, and an exception occurs when OMS pulls the incremental data from a standby Oracle database, you can run the `ALTER SYSTEM SWITCH LOGFILE` command in the primary database to handle the error.

  

* When you migrate a table without the primary key from an Oracle database to a MySQL tenant of OceanBase Database, do not perform any operations on the source Oracle database that may change the ROWID, such as data import and export, Alter Table, FlashBack Table, and partition splitting or compaction.

  

* Chinese characters are not allowed for the name of the table to be migrated and column names.

  

* The data of the NUMERIC type in the MySQL tenant of OceanBase Database cannot serve as a partitioning key. During schema migration of a partitioned table without a primary key, the data of the NUMBER or INT type in the partition column of the Oracle database is converted to data of the NUMERIC type, resulting in an error.

  

* During schema migration, the data of the TIMESTAMP type (with a precision of 9) in the Oracle database is converted to data of the DATETIME type (with a precision of 6) in the MySQL tenant of OceanBase Database. Precision loss occurs.

  

* During schema migration, the data of the BINARY_FLOAT type in the Oracle database is converted to data of the DOUBLE type in the MySQL tenant of OceanBase Database. Precision loss may occur during reverse incremental migration.

  

* If you do not specify the **DDL for Schema Change** field when you create the project, you must comply with the following rules to add or delete columns in the source database:

  * Add a column in the destination database first and then in the source database.

    
  
  * Delete a column in the destination database first and then in the source database.

    
  

  

* During reverse incremental migration from an Oracle database to a MySQL tenant of OceanBase Database earlier than V3.2x, the `row movement` operation causes the following problems:

  In a multi-partition table with a globally unique index, if you update the value of the partitioning key of the table, data will be deleted from the old partitions and inserted to the new partitions in the MySQL tenant of OceanBase Database. liboblog cannot determine the dependency of data changes between partitions during migration and may output the INSERT logs first and then the DELETE logs to OMS. OMS may insert new data in the destination database during the replay of the INSERT logs but then delete the inserted data during the replay of the DELETE logs due to the unique index. This results in data loss during the migration process.
  

* When you migrate data from an Oracle database to a non-Oracle tenant of OceanBase Database, if the primary key or unique key (as a verification field) contains data of the INTERVAL type, you must set `filter.verify.inmod.tables` to the in mode for the table. Otherwise, the verification result is inaccurate.

  




Check and modify the system configurations of the Oracle instance 
--------------------------------------------------------------------------------------

1. Enable archivelog for the source Oracle database. 

   ```sql
   select log_mode from v$database;
   ```

   

   The value of the `log_mode` field must be `archivelog`. Otherwise, perform the following steps to change it: 
   1. Run the following commands to enable archivelog. 

      ```sql
      shutdown immediate;
      startup mount;
      alter database archivelog;
      alter database open;
      ```

      
   
   2. Run the following command to view the path and quota of archived logs: 

      View the path and quota of the `reover file`. We recommend that you set the `db_recovery_file_dest_size` parameter to a relatively large value. After you enable archivelog, you need to regularly clear the archived logs by using RMAN or other methods. 

      ```sql
      show parameter db_recovery_file_dest;
      ```

      
   
   3. Change the quota of archived logs as needed: 

      ```sql
      alter system set db_recovery_file_dest_size=50G scope=both;
      ```

      
   

   

2. Enable supplemental_log in the source Oracle database. 

   LogMiner Reader allows you to enable only table-level supplemental_log in an Oracle database. If you create new tables in the Oracle instance before the migration, you must enable the supplemental_log for the primary key and unique key before the DML operations. Otherwise, OMS returns an exception of incomplete logs. 
   **Notice**

   

   You need to enable supplemental_log in the primary Oracle database.

   LogMiner Reader uses one of the following two methods to check whether supplemental_log is enabled. If not, LogMiner Reader exits. 
   * Enable `supplemental_log_data_pk` and `supplemental_log_data_ui` at the database level. 

     Run the following command to check whether the supplemental_log is enabled. If the returned values are both `YES`, the supplemental_log is enabled. 

     ```sql
     select supplemental_log_data_pk, supplemental_log_data_ui from v$database;
     ```

     

     Otherwise, perform the following steps:
     1. Execute the following statement to enable the supplemental_log. 

        ```sql
        alter database add supplemental log data(primary key, unique) columns;
        ```

        
     
     2. Perform archivelog switchover three times. If you use the RAC log system, alternately perform the archivelog switchover among multiple instances. 

        ```sql
        alter system switch logfile;
        ```

        

        The reason for performing the archivelog switchover three times:

        When the Oracle Store locates the start time to pull log files, it rolls back 0 to 2 archived logs based on the specified timestamp. Therefore, after you enable the supplemental_log, you need to perform the archivelog switchover three times to prevent the Store from pulling the logs that are generated before the specified timestamp. Otherwise, the Store exits unexpectedly. 

        The reason for alternately performing the archivelog switchover among multiple instances in an RAC system:

        In an RAC system, if you perform the archivelog switchover multiple times on one instance, when you perform the archivelog switchover on the next instance, the latter instance may pull the logs that are generated before the supplemental_log is enabled.
        
     

     
   
   * Enable `supplemental_log_data_pk` and `supplemental_log_data_ui` at the table level. 

     1. Execute the following statement to confirm whether `supplemental_log_data_min` is enabled at the database level. 

        ```sql
        select supplemental_log_data_min from v$database;
        ```

        

        If the returned value is `YES` or `IMPLICIT`, the supplemental_log is enabled.
        
     
     2. Execute the following statement to check whether the table-level supplemental_log is enabled for the tables to be synchronized. 

        ```sql
        select log_group_type from all_log_groups where owner = 'xxx' and table_name = 'yyy';
        ```

        

        Each type of supplemental_log returns one row. The results must contain `ALL COLUMN LOGGING` or both `PRIMARY KEY LOGGING` and `UNIQUE KEY LOGGING`. 

        If the table-level supplemental_log is not enabled, execute the following statement: 

        ```sql
        alter table table_name add supplemental log data(primary key, unique) columns;
        ```

        
     
     3. Perform archivelog switchover three times. If you use the RAC log system, alternately perform the archivelog switchover among multiple instances. 

        ```sql
        alter system switch logfile;
        ```

        
     

     
   

   

3. Set the `_log_parallelism_max` parameter of the Oracle database to 1. By default, the parameter is set to 2. 

   You can use one of the following two methods to query the value of the `_log_parallelism_max` parameter: 
   * Method 1

     ```sql
     SELECT NAM.KSPPINM,VAL.KSPPSTVL,NAM.KSPPDESC FROM SYS.X$KSPPI NAM,SYS.X$KSPPSV VAL WHERE NAM.INDX= VAL.INDX AND NAM.KSPPINM LIKE '_%' AND UPPER(NAM.KSPPINM) LIKE '%LOG_PARALLEL%';
     ```

     
   
   * Method 2

     ```sql
     select value from v$parameter where name = '_log_parallelism_max';
     ```

     
   

   

   Execute one of the following statements to modify the value of the `_log_parallelism_max` parameter.
   * If you use the RAC log system, execute the following statement:

     ```sql
     alter system set "_log_parallelism_max"=1 sid='*' scope=spfile;
     ```

     
   
   * If you do not use the RAC log system, execute the following statement:

     ```sql
     alter system set "_log_parallelism_max"=1 scope=spfile;
     ```

     
   

   

   When you modify the value of the `_log_parallelism_max` parameter in Oracle Database 10g, if the error message `write to SPFILE requested but no SPFILE specified at startup` is returned, perform the following operations:

   ```sql
   create spfile from pfile;
   shutdown immediate;
   startup;
   show parameter spfile;
   ```

   

4. Restart the instance and perform the archivelog switchover three times.

   




Create a data migration project 
----------------------------------------------------

1. Create a migration project. 

   1. Log on to the OMS console.

      
   
   2. In the left-side navigation pane, click **Data Migration** .

      
   
   3. On the **Data Migration** page, click **Create Migration Project** in the upper-right corner.

      
   

   

2. On the **Select the source and the destination** page, specify the fields. 

   

   |         Field          |                                                                                                                                                                                                                                                                                                                                                    Description                                                                                                                                                                                                                                                                                                                                                    |
   |------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
   | Migration Project Name | The name must not exceed 64 characters in length and can contain Chinese characters, digits, and letters.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
   | Tag                    | Click the field and select a tag from the drop-down list. You can also click **Manage Tags** to create, modify, or delete tags. For more information, see [Manage data migration projects by using tags](/en-US/5.user-guide/5.data-migration-1/6.migration-project-management/6.manage-project-tags.md).                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
   | Source Node            | If you have created Oracle data sources, select one from the drop-down list. Otherwise, click **Add Data Source** in the drop-down list, and add a data source in the dialog box on the right. For more information, see [Add an Oracle data source](/en-US/5.user-guide/3.manage-data-sources/1.add-a-data-source/3.add-an-oracle-data-source.md).                                                                                                                                                                                                                                                                                                                                                                                                                |
   | Destination Node       | If you have created MySQL tenants in OceanBase Database as data sources, select one from the drop-down list. Otherwise, click **Add Data Source** in the drop-down list, and add a data source in the dialog box on the right. For more information, see [Add OceanBase Database physical tables as data source](/en-US/5.user-guide/3.manage-data-sources/1.add-a-data-source/1.add-an-oceanbase-data-source-1/1.add-an-oceanbase-data-source.md).  **Notice**  If the destination data source is not bound to an OCP cluster, the system prompts that the data migration project does not support reverse incremental migration. To enable reverse incremental migration, click **Edit Data Source** next to the prompt, and bind the data source to an OCP cluster. |

   

3. Click **Next** .

   

4. On the **Select migration types and objects** page, specify **Migration Type** for the migration task based on your business needs. 

   Migration types include **Schema Migration** , **Full Migration** , **Incremental Migration** , **Full Verification** , and **Reverse Incremental Migration** . If the MySQL tenant of OceanBase Database is not bound to an OCP cluster, you cannot select **Reverse Incremental Migration** . 

   If you select **Full Migration** , we recommend that you select **Full Verification** as well. After the incremental data is synchronized to the destination database, a full verification task is initiated for the migrated data tables in the source and destination databases. 

   **Incremental Migration** supports the following DML operations for data change: `Insert`, `Delete`, and `Update`. You can select the operations based on your business needs.
   

5. On the **Select migration types and objects** page, select the migration objects. 

   You can select the migration objects in one of the following two ways: **Specified object** and **Matching rules** . 
   * If you select **Specified object** , select the objects to be migrated on the left, and click **\>** to add them to the list on the right. You can select tables and views of one or more databases for migration. 

     **Notice**

     
     * Chinese characters are not allowed for the name of the table to be migrated and column names.

       
     
     * If the database or table name contains a double dollar sign ($$), you cannot create the migration project.

       
     

     

     When you migrate data from an Oracle database to a MySQL tenant of OceanBase Database, OMS allows you to import objects through text, rename object names, set row filters, view column information, and remove one or all objects to be migrated. 
     

     |      Action       |                                                                                                                                                                                                                                                                                                                                                                                                                             Steps                                                                                                                                                                                                                                                                                                                                                                                                                             |
     |-------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
     | Import Objects    | 1. In the list on the right of the **Data Table** section, click **Import Objects** in the upper-right corner.   2. In the dialog box that appears, click **OK** .  **Notice**  This operation will overwrite previous selections. Proceed with caution.   3. In the **Import Migration Object** dialog box, enter the objects to be migrated, for example, `SCHEMA.TB1 | SCHEMA.TB2 |`.  We recommend that you migrate no more than 10,000 objects at a time.   4. Click **Validate** .   5. After the validation succeeds, click **OK** .                                                                            |
     | Rename            | 1. In the list on the right of the **Data Table** section, move the pointer over the target object.   2. Click **Rename** .   3. Enter a new name and click **OK** .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
     | Settings          | OMS allows you to set `WHERE` conditions to filter data by row and view column information.  1. In the list on the right of the **Data Table** section, move the pointer over the target object.   2. Click **Settings** .   3. In the **Settings** dialog box, enter a `WHERE` clause of a standard SQL statement to configure row filtering.  Only the data meeting the `WHERE` condition is synchronized to the destination data source, thereby filtering data by row. Add escape characters (\`) for keywords of data sources.   4. Click **OK** .  You can also view column information of the migration object in the **View Column** section.    |
     | Remove/Remove All | OMS allows you to remove one or all migration objects.  * Remove a single migration object In the list on the right of the **Data Table** section, move the pointer over the target object, and click **Remove** . The migration object is removed.   * Remove all migration objects In the list on the right of the **Data Table** section, click **Remove All** in the upper-right corner. In the dialog box that appears, click **OK** to remove all migration objects.                                                                                                                                                                                                                                                 |

     

     When the source database is an Oracle database, if row filtering is enabled for columns other than the primary key and unique key columns, enable supplemental_log for the corresponding columns or all columns. 

     Methods for enabling supplemental_log for the corresponding columns:

     ```sql
     ALTER TABLE table_name ADD SUPPLEMENTAL LOG GROUP log_group_name (column1, column2, column3) ALWAYS;
     ```

     

     Methods for enabling supplemental_log for all columns:

     ```sql
     -- Enable database-level supplemental_log:
     ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;
     -- Enable table-level supplemental_log:
     ALTER TABLE table_name ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;
     ```

     
   
   * If you select **Matching rules** , you must configure matching rules. For more information, see [Configure matching rules for migration objects](/en-US/5.user-guide/5.data-migration-1/7.set-a-blacklist-and-a-whitelist.md).

     
   

   

6. Click **Next** . On the **Migration Options** page, specify the fields. 

   

   |     Category      |                                  Field                                   |                                                                                                                                                       Description                                                                                                                                                        |
   |-------------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
   | Basic Settings    | Concurrency for Full Migration                                           | The value can be **Smooth** , **Normal** , or **Fast** . Full data migration tasks with different performance settings consume different resources.  You can also modify the configurations of the specific Checker to customize the concurrency.                                                        |
   | Basic Settings    | Full Verification Concurrency                                            | The value can be **Smooth** , **Normal** , or **Fast** . Different quantities of resources of the source and destination databases are consumed at different concurrencies.  You can also modify the configurations of the specific Checker to customize the concurrency.                                |
   | Basic Settings    | Incremental Record Retention Time                                        | The duration that incremental parsed files are cached in OMS. A longer retention period indicates more disk space occupied by the Store component of OMS.                                                                                                                                                                |
   | Advanced Settings | Whether to Allow Destination Table to Be Not Empty During Full Migration | When a non-empty destination table is allowed for full migration, full verification runs based on the `in` condition and does not need to be removed.                                                                                                                                                                    |
   | Advanced Settings | Whether to Allow Post-indexing                                           | You can specify whether to allow post-indexing after full migration is completed. Post-indexing can shorten the time of full migration.  **Notice**  To enable this feature, select both **Schema Migration** and **Full Migration** on the **Select migration types and objects** page. |

   

7. Click **Precheck** to start precheck on the data migration project. 

   During the precheck, OMS checks whether the database user has the read and write privileges and whether the network connections of the databases meet the requirements. A data migration project can be started only after it passes all check items. If the precheck fails, identify the cause, fix the problem, and run the precheck again until it succeeds. You can modify the system configurations to skip a precheck item.
   

8. Click **Start Task** to start tasks of the project such as schema migration and full migration. 

   If you do not want to immediately start the project, click **Save** to go to the details page of the data migration project. You can start the project later as needed. For more information about project details, see [View details of a data migration project](/en-US/5.user-guide/5.data-migration-1/6.migration-project-management/1.view-migration-task-details.md).
   




Start a data migration project 
---------------------------------------------------

You can start a data migration project only after it passes all check items. If any check item fails, fix the problem manually and perform the check again. After the data migration project starts, run the selected migration types in sequence:

1. Schema migration

   The migration of the definitions of data objects (tables, indexes, constraints, comments, and views) from the source database to the destination OceanBase database. Temporary tables are automatically filtered out. 

   If the source database is not an OceanBase database, OMS performs format conversion and encapsulation based on the syntax definition and standard of the tenant type of the destination OceanBase database, and then replicates the data to the destination OceanBase database. 

   You can view the migration progress of tables and views and perform the following operations on the target object:
   * **View Creation Syntax** : View table creation syntax and modify index creation syntax. 

     Fully compatible DDL syntax executed on the OBServer is displayed. Incompatible syntax is converted before it is displayed.
     
   
   * **Modify the creation syntax and try again** : Check the error information and modify the definition of the conversion result of a failed DDL statement, and then migrate the data to the destination again.

     
   
   * **View Database Return Code** : View the DDL statements executed on the OBServer and the execution error information of a failed schema migration task.

     
   
   * **Retry** / **Retry All Failed Objects** : Retry failed schema migration tasks one by one or retry all failed tasks at a time.

     
   
   * **Skip** / **Batch Skip** : Skip failed schema migration tasks one by one or skip multiple failed tasks at a time. To skip multiple objects at a time, click **Batch Skip** in the upper-right corner. When you skip an object, its index is also skipped.

     
   
   * **Remove** / **Batch Remove** : Remove failed schema migration tasks one by one or remove multiple failed tasks at a time. To remove multiple failed tasks at a time, click **Batch Remove** in the upper-right corner. When you remove an object, its index is also removed.

     
   

   

2. Full migration

   The migration of the existing data from tables in the source database to corresponding tables in the OceanBase database. You can view table objects and table indexes on the **Full Migration** page. The status of the full migration changes to Completed only after migration of the table objects and table indexes is completed. On the **Table Indexes** page, you can click **View Creation Syntax** next to the target table object to view its index creation syntax. 

   You can combine full migration with incremental migration to ensure data consistency between the source and destination databases. If any objects failed to be migrated during a full migration, the causes of the failure are displayed. 

   Full data migration inherits the database and table mapping rules that were configured when the project was created. For more information, see [Data types and syntax conversion](/en-US/5.user-guide/5.data-migration-1/2.data-types-and-syntax-conversion.md).
   

3. Incremental migration

   The migration of changed data of the source database to the corresponding table in the destination OceanBase database. Data changes include data addition, modification, and deletion. 

   When services are continuously writing data to the source database, OMS starts the incremental data pull module to pull incremental data from the source instance, parses and encapsulates the incremental data, and then stores the data in OMS, before it starts the full data migration. 

   After a full data migration task is completed, OMS starts the incremental data replay module to pull incremental data from the incremental data pull module. The incremental data is synchronized to the destination instance after being filtered, mapped, and converted. 

   In the incremental migration section, you can view the performance information of the incremental migration task, such as the migration latency, synchronization timestamp, and migration traffic. 

   When a data migration project is in the **Paused** or **Failed** state, you can modify **Current Timestamp** . When you modify the timestamp for incremental migration, you cannot select a timestamp later than the current time, and the modification may cause data duplication or loss. Proceed with caution.
   

4. Full verification

   After the full migration and incremental migration are completed, OMS automatically initiates a full data verification task to verify the data tables in the source database and the tables in the destination database. 

   OMS also provides corresponding APIs for you to initiate custom data verification tasks in the incremental data synchronization process. 

   You can view the information of specific columns where inconsistent data is detected. OMS runs SQL scripts to correct the data in the destination database based on the data in the source database. The correction operation is not supported if the source database has no corresponding data. 

   OMS allows you to skip full data verification for migration tasks in the **Executing** state. On the **Full Verification** page, click **Skip Full Verification** . In the dialog box that appears, click **Yes** . 

   After the full verification is completed, you can click **Go To Next Stage** to start the forward switchover. After you enter the switchover process, you cannot re-check the current verification task for data comparison and correction.
   

5. Forward switchover

   1. Start forward switchover

      In this step, the migration does not stop. You only confirm the switchover process that is about to start. To start the forward switchover task, click **Start Forward Switchover** . 
      **Notice**

      
      Before you start a forward switchover task, make sure that the writing status of the source data source is either Stopping or Stopped.
      
   
   2. Perform switchover precheck

      Check whether the current project status supports switchover. The precheck involves the following steps:
      * Check the synchronization latency: If the synchronization latency is within 15 seconds after incremental synchronization is started, the switchover passes this check item. If incremental synchronization is not started, the switchover automatically passes this check item.

        
      
      * Check the user write permission on the source side.

        
      

      

      If the switchover passes the precheck, the system automatically performs the next step. If the switchover fails the precheck, the system shows the error details. 

      In this case, you can **Retry** or **Skip** the precheck. 

      After you click **Skip** , you need to click **Skip** again in the dialog box that appears.
      
   
   3. Start the destination Store

      Start incremental data pulling in the destination database. Create and start a destination Store. If the start fails, you can choose to click **Retry** or **Skip** .
      
   
   4. Confirm that writing is stopped in the source database

      In the **Confirm Writing Stopped at Source** section, click **OK** to make sure that no incremental data is generated in the source database.
      
   
   5. Confirm the writing stop timestamp upon synchronization completion

      OMS automatically checks whether the source and destination databases are synchronized to the same timestamp. After the check is completed, the latency and timestamp of the incremental synchronization are displayed.
      
   
   6. Stop forward synchronization

      Stop the JDBCWriter from the source database to the destination database. If the forward synchronization fails to be stopped, you can choose to click **Retry** or **Skip** .
      
   
   7. Process database objects

      In this step, the database objects are migrated, the additional columns and indexes added by OMS are deleted, and the constraints that are automatically ignored during the schema migration are added. You also need to confirm that objects such as triggers and sequences have been manually migrated and that the triggers and foreign keys of the source are disabled. 

      You need to click **Run** to process the database objects. For a running project, you can click **View Logs** or **Skip** . For projects that have been processed, you need to click **Mark as Complete** . After all projects have been marked as completed, proceed to the next step.
      
   
   8. Start reverse incremental migration

      In the **Start Reverse Incremental Migration** section, click **Start Reverse Incremental Migration** to start the JDBCWriter from the destination database to the source database. Wait until the **Successfully started reverse increment** message appears.
      
   

   

6. After reverse incremental migration is started, click the **Reverse Incremental Migration** tab to view detailed information such as **Start Time** and **Reverse Synchronization Performance** . 

   Performance metrics of reverse incremental migration:
   * Latency: The time consumed to synchronize changed incremental data from the destination database to the source database in the unit of seconds.

     
   
   * Migration traffic: The traffic throughput of incremental data synchronization from the destination database to the source database in the unit of MB/s.

     
   

   



