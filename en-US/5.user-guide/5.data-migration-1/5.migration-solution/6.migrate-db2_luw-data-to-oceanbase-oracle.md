Create a project to migrate data from a DB2 LUW database to an Oracle tenant of OceanBase Database 
=======================================================================================================================

OceanBase Migration Service (OMS) allows you to migrate data from a DB2 LUW database to an Oracle tenant of OceanBase Database. OMS supports schema migration, full migration, incremental migration, full verification, and reverse incremental migration. 

Background information 
-------------------------------------------

You can create a data migration project in the OMS console to migrate the existing business data and incremental data from a DB2 LUW database to an Oracle tenant of OceanBase Database through schema migration, full migration, and incremental migration.

Prerequisites 
----------------------------------

* You have created a corresponding schema in the destination Oracle tenant of OceanBase Database. 

  You must create a corresponding schema in the destination database before migration. OMS will migrate the tables and views to the schema.
  

* You have created dedicated database users in the source DB2 LUW database and the destination Oracle tenant of OceanBase Database for data migration and granted the corresponding privileges to the users. For more information, see [Create and authorize a database user](/en-US/5.user-guide/5.data-migration-1/4.preparation/1.create-and-authorize-a-database-user.md).

  

* You have enabled archivelog for the DB2 LUW database. 

  You can perform the following steps to enable archivelog: 
  1. Connect to the database. 

     ```shell
     db2 connect to ${db_name}
     ```

     
  
  2. Modify the directory of archived logs. 

     ```shell
     db2 update db cfg for ${db_name} using LOGARCHMETH1 logpath(${your_logpath})
     ```

     
  
  3. Back up the logs. 

     ```shell
     db2 backup database ${db_name} to logbackuppath(${your_logpath})
     ```

     
  
  4. Stop the database. 

     ```shell
     db2stop
     ```

     
  
  5. Start the database. 

     ```shell
     db2start
     ```

     
  
  6. Connect to the database. 

     ```shell
     db2 connect to ${db_name}
     ```

     
  
  7. Manually archive logs. 

     ```shell
     db2 archive log for db ${db_name}
     ```

     
  
  8. View archived logs. 

     ```shell
     db2 get db cfg|grep LOG
     ```

     
  

  

* You have enabled Data Changes for the table in the Store of the DB2 LUW database. 

  You can execute the following statement to enable Data Changes: 

  ```shell
  alter table ${table_name} data capture changes
  ```

  

* You have enabled `log_ddl_stmt` for the DB2 LUW database. 

  You can execute the following statement to enable `log_ddl_stmt`: 

  ```shell
  db2 update db cfg using LOG_DDL_STMTS YES
  ```

  




Limits 
---------------------------

* The DB2 LUW database can only parse an object whose name consists of letters, underscores(_), and digits, begins with a letter or underscore, and does not contain a DB2 LUW keyword.

  

* DB2 LUW 10.5, 11.1, and 11.5 for Linux and AIX are supported. You can also migrate data from DB2 LUW 10.1 to an Oracle tenant of OceanBase Database V2.2.76 or a later version.

  

* Only tables with unique constraints in the DB2 LUW database support full migration and incremental migration.

  

* After you perform a DDL operation on the source DB2 LUW database, we recommend that you run the `CALLSYSPROC.ADMIN_CMD('REORG TABLE ZZ_TABLE')` command.

  

* If you enable DDL operations for incremental data migration, the `drop index` command is executed on all indexes, which may cause index loss in the destination database.

  

* When data of the LOB type in the DB2 LUW database is updated, a large number of row migrations for logs are generated compared with common scenarios. If an unknown combination of row migrations causes an unexpected exit of the Store, retain the logs for OceanBase Database Technical Support.

  

* You cannot migrate tables without unique constraints from the DB2 LUW database to an Oracle tenant of OceanBase Database.

  

* Do not use the UPDATE operation to change the primary key. Otherwise, data inconsistency may occur during row migration.

  

* The DB2 LUW database is mainly used to pull uncompressed logs for testing. The stability of pulling compressed logs has not been verified. Therefore, pull compressed logs with caution.

  

* Retain logs of the DB2 LUW database and OceanBase Database for at least three days in case of unexpected pullback.

  

* A column with a unique constraint cannot be null. Otherwise, data inconsistency may occur. Multiple null values are allowed in the same column with a unique constraint in OceanBase Database: null!=null. However, a column with a unique constraint in the DB2 LUW database cannot be null. A unique index allows null values, but null=null. 

  For example, the unique (c1, c2) (null, null) index of OceanBase Database can be inserted multiple times, while a unique constraint of the DB2 LUW database does not allow null values. If a unique index is used, (null, null) can be inserted only once. 

  Therefore, incompatibility occurs due to null values when OceanBase Database uses unique indexes. Do not use unique keys that allow null columns. Otherwise, errors can occur during schema migration. During incremental migration, the NOT NULL constraint is added to the corresponding column, and an error occurs when a null value is written to the column. 

  In addition, if you create a unique index in OceanBase Database during a DDL operation for incremental migration, make sure that all columns with unique indexes are not null. Otherwise, the DB2 LUW database returns an error.
  

* The user parsed by the DB2 LUW database must have the `sysadm` privilege on the corresponding schema. Otherwise, the user cannot obtain logs.

  

* Chinese characters are not allowed for the name of the table to be migrated and column names.

  

* If the NOT NULL constraint is added to the schema of the destination Oracle tenant of OceanBase Database, null strings generated by the source DB2 LUW database cannot be written to the destination.

  

* If you do not specify the **DDL for Schema Change** field when you create the project, you must comply with the following rules to add or delete columns in the source database:

  * Add a column in the destination database first and then in the source database.

    
  
  * Delete a column in the destination database first and then in the source database.

    
  

  

* During reverse incremental migration from a DB2 LUW database to an Oracle tenant of OceanBase Database earlier than V3.2x, the `row movement` operation causes the following problems:

  In a multi-partition table with a globally unique index, if you update the value of the partitioning key of the table, data will be deleted from the old partitions and inserted to the new partitions in the Oracle tenant of OceanBase Database. liboblog cannot determine the dependency of data changes between partitions during migration and may output the INSERT logs first and then the DELETE logs to OMS. OMS may insert new data in the destination database during the replay of the INSERT logs but then delete the inserted data during the replay of the DELETE logs due to the unique index. This results in data loss during the migration process.
  

* If the `rename` operation is performed during incremental migration and the source or destination table is not in the synchronization list, the `rename` operation is ignored. After the `rename` operation is completed, restart full verification. The full verification will fail because the renamed table is not synchronized to the destination database.

  




Data type mappings 
---------------------------------------

### Conversion rules for schema migration 



|                                                                                                                                             DB2 LUW database                                                                                                                                             |                                                                      Oracle tenant of OceanBase Database                                                                      |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| TIME                                                                                                                                                                                                                                                                                                     | DATE **Warning**  If the default values are incompatible, modify them.                                                                        |
| TIMESTAMP(n)                                                                                                                                                                                                                                                                                             | TIMESTAMP(n\>0)                                                                                                                                                               |
| DATE                                                                                                                                                                                                                                                                                                     | DATE                                                                                                                                                                          |
| * Version 10.1: CHAR (n)   * Version 10.5 and later versions: CHAR (n OCTETS\|CODEUNITS32)    **Notice**  Only DB2 LUW 10.5 and later versions support the OCTETS and CODEUNITS32 encoding units.     | * DB2 LUW 10.1: CHAR(n)   * DB2 LUW 10.5 and later versions: CHAR(n BYTE\|CHAR)            |
| CHAR(n) FOR BIT DATA                                                                                                                                                                                                                                                                                     | RAW(n\<=255)                                                                                                                                                                  |
| * Version 10.1: VARCHAR(n)   * Version 10.5 and later versions: VARCHAR(n OCTETS\|CODEUNITS32)    **Notice**  Only DB2 LUW 10.5 and later versions support the OCTETS and CODEUNITS32 encoding units. | * DB2 LUW 10.1: VARCHAR2(n)   * DB2 LUW 10.5 and later versions: VARCHAR2(n BYTE\|CHAR)    |
| VARCHAR(n) FOR BIT DATA                                                                                                                                                                                                                                                                                  | RAW(n\<=2000) or BLOB                                                                                                                                                         |
| NCHAR(m)                                                                                                                                                                                                                                                                                                 | NCHAR(m)                                                                                                                                                                      |
| NVARCHAR(m)                                                                                                                                                                                                                                                                                              | NVARCHAR2(m)                                                                                                                                                                  |
| CLOB                                                                                                                                                                                                                                                                                                     | CLOB                                                                                                                                                                          |
| NCLOB                                                                                                                                                                                                                                                                                                    | CLOB                                                                                                                                                                          |
| GRAPHIC(n)                                                                                                                                                                                                                                                                                               | NCHAR(n)                                                                                                                                                                      |
| VARGRAPHIC(n)                                                                                                                                                                                                                                                                                            | NVARCHAR2(n)                                                                                                                                                                  |
| LONG VARGRAPHIC                                                                                                                                                                                                                                                                                          | CLOB                                                                                                                                                                          |
| LONG VARCHAR                                                                                                                                                                                                                                                                                             | VARCHAR2(m BYTE)                                                                                                                                                              |
| DBCLOB                                                                                                                                                                                                                                                                                                   | CLOB                                                                                                                                                                          |
| BINARY(m \< 256)                                                                                                                                                                                                                                                                                         | RAW                                                                                                                                                                           |
| VARBINARY(m \< 32672)                                                                                                                                                                                                                                                                                    | BLOB                                                                                                                                                                          |
| BLOB                                                                                                                                                                                                                                                                                                     | BLOB                                                                                                                                                                          |
| BOOLEAN                                                                                                                                                                                                                                                                                                  | NUMBER(1)                                                                                                                                                                     |
| SMALLINT                                                                                                                                                                                                                                                                                                 | NUMBER(6, 0)                                                                                                                                                                  |
| INTEGER                                                                                                                                                                                                                                                                                                  | NUMBER(11,0)                                                                                                                                                                  |
| BIGINT                                                                                                                                                                                                                                                                                                   | NUMBER(19, 0)                                                                                                                                                                 |
| DECIMAL(p,s)                                                                                                                                                                                                                                                                                             | NUMBER(p,s)                                                                                                                                                                   |
| NUMERIC(p,s)                                                                                                                                                                                                                                                                                             | NUMBER(p,s)                                                                                                                                                                   |
| DECFLOAT(16\|34)                                                                                                                                                                                                                                                                                         | FLOAT(53\|113)                                                                                                                                                                |
| REAL                                                                                                                                                                                                                                                                                                     | BINARY_FLOAT                                                                                                                                                                  |
| DOUBLE                                                                                                                                                                                                                                                                                                   | BINARY_DOUBLE                                                                                                                                                                 |
| XML                                                                                                                                                                                                                                                                                                      | --                                                                                                                                                                            |


**Notice**



* The CHAR and VARCHAR2 data types in an Oracle tenant of OceanBase Database can store multi-byte encoded data. Therefore, if single-byte encoding units are used in reverse conversion, the data types may be not long enough.

  

* In a DB2 LUW database, the lengths of data types as well as the OCTETS, CODEUNITS16, and CODEUNITS32 encoding units must be considered for data storage. 

  Only DB2 LUW 10.5 and later versions support the OCTETS and CODEUNITS32 encoding units.
  

* Data of the CLOB and BLOB types must be less than 48 MB in size.

  

* Data of the LONG, ROWID, BFILE, LONG RAW, XMLType, and UDT types cannot be migrated.

  




### Limits 

* The maximum timestamp precision of a DB2 LUW database is 12, while that of an Oracle tenant of OceanBase Database is 9. Therefore, the data is truncated when it is migrated from a DB2 LUW database to an Oracle tenant of OceanBase Database. The data that will be truncated cannot be used as the primary key or unique key.

  

* Length limits

  * The data of the CHAR or BINARY type cannot exceed 255 bytes in length in a DB2 LUW database. If the data written to an Oracle tenant of OceanBase Database exceeds 255 bytes in length, an error will occur during reverse synchronization.

    
  
  * The data of the VARCHAR or BINARY type cannot exceed 32,673 bytes in length in a DB2 LUW database. If the data written to an Oracle tenant of OceanBase Database exceeds 32,673 bytes in length, an error will occur during migration.

    
  
  * The Decimal(dp, ds) in a DB2 LUW database is equivalent to the NUMBER of an Oracle tenant of OceanBase Database. The length of dp cannot exceed 31 and must be greater than that of ds. 

    The number written to an Oracle tenant of OceanBase Database cannot exceed the maximum allowed number. By default, the data of the NUMBER, INT, SMALLINT, and NUMBER(\*, s) types is 38 bytes in length in an Oracle tenant of OceanBase Database. You need to explicitly define the NUMBER(p,s) to the length that is compatible with the business application and the source and destination databases.
    
  

  

* Data type limits

  * Note that when a data type in a DB2 LUW database is converted to the LOB type in an Oracle tenant of OceanBase Database, the data of the LOB type cannot exceed 48 MB in size.

    
  
  * The data of the TIME type in a DB2 LUW database cannot be migrated as the partitioning key.

    
  
  * The data of the XML type is not supported.

    
  
  * We do not recommend that you use CODEUNITS16 or CODEUNITS32 to define or store multi-byte data of the NCHAR, GRAPHIC, or other types.

    
  
  * The default value of the BLOB-type data cannot be modified.

    
  

  




Dynamic DDL operations 
-------------------------------------------

* Supported DDL operations

  * `create table`

    
  
  * `alter table`

    
  
  * `drop table`

    
  
  * `truncate table`

    
  
  * `rename table`

    
  
  * `create index`

    
  
  * `drop index`

    
  
  * Operations for adding and removing comments of tables or columns

    
  

  

  The following DDL operations are supported when you migrate incremental data from DB2 LUW 10.1 to an Oracle tenant of OceanBase Database V2.2.76 or a later version:
  * `create table`

    
  
  * `drop table`

    
  
  * `truncate table`

    
  
  * `rename table`

    
  
  * `add column`

    
  
  * `modify column`

    
  

  

* Limits

  * Create Table

    

    |          Category           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
    |-----------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
    | Syntax restriction          | * The Create like syntax is not supported.   * Syntax that contains column type aliases (non-native data types) is not supported.   * Syntax that contains the Select or Subquery statement is not supported.   * An error may be returned if the generated column contains incompatible functions or complex expressions.   * Syntax that contains the XML data type is not supported.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
    | Column attribute processing | * Column names, column types, null flag bits, and default values are processed. Other column attribute values are ignored.   * DDL statements do not support attributes of generated columns and do not process the definitions of generated columns.   * Default values with functions should be used with caution. If the functions do not exist in the destination database, the table fails to be created.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
    | Index processing            | * We recommend that you explicitly define indexes, including primary keys, unique keys, and CHECK constraints. CHECK constraints are not preferred. Anonymous indexes automatically generate names, which may cause name conflicts.   * Functions should be used with caution. If the functions do not exist in the destination database, the table fails to be created.   * Foreign keys are not recommended. OMS does not support synchronization of tables with foreign keys. Foreign keys will be discarded during DDL conversion.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
    | Partition                   | * OMS supports synchronization of time-based range partitions but directly translates expressions for non-time-based range partitions. We do not recommend that you use functions in partitions. Functions may cause incompatibility of syntax and further cause failure of the migration project.   * The range of a partition cannot start or end with a representation related to `MINVALUE` or `MAXVALUE`.   * Partitions of the DB2 LUW database must contain the `ending` field with the `exclusive` attribute. Otherwise, an error is returned.   * The DB2 LUW database does not support the `every` syntax extension for partitions to prevent calculation errors .   * In the DB2 LUW database, a partition column is added to the primary key of a table. If the primary key and partitioning key have no intersection, the primary key is changed to a unique index to adapt to the partition rules and constraints of OceanBase Database.   * In the DB2 LUW database, the `create/alter table add partition` statement must contain a partition name. Anonymous partitions are not supported.    |

    
  
  * Alter Table

    * The `alter table add primary` statement in a DB2 LUW database is converted to the `add unique` statement in an Oracle tenant of OceanBase Database. OceanBase Database does not support adding a primary key after a table is created. Therefore, you must explicitly specify the primary key when you create a table.

      
    
    * When you execute the `alter table` statement to modify data of the LOB type, you must specify the data length. However, you cannot reduce the length of the LOB-type data by executing the `alter table` statement.

      
    
    * The `unique index` cannot be empty in a DB2 LUW database. Therefore, the column that is specified as the unique key requires NOT NULL constraint. In other words, when you execute the `alter table add unique` statement, you must specify the NOT NULL constraint for the related column.

      
    

    
  
  * Create Index

    You cannot specify Local in the statement to create a regular table in OceanBase Database.
    
  
  * Parse DDL statements

    When you parse DDL statements, you can refer to the official DB2 database documentation to ensure the maximum compatibility. 

    Note the following limits when you parse the source DDL statements in a DB2 LUW database:
    * The DB2 LUW database checks the schema to refresh the cached, parsed source DDL statements. We recommend that you execute only one type of DDL statements at a time and proceed after you confirm the results.

      
    
    * Avoid frequent `create` and `drop` operations on partitioned tables. We recommend that you execute the `create partition table` statement, confirm that the table is synchronized, and then execute the `drop partition table` statement.

      
    
    * The DB2 LUW database can only parse an object whose name consists of letters, underscores(_), and digits, begins with a letter or underscore, and does not contain a DB2 LUW keyword.

      
    

    
  

  




Create a data migration project 
----------------------------------------------------

1. Create a data migration link. 

   1. Log on to the OMS console.

      
   
   2. In the left-side navigation pane, click **Data Migration** .

      
   
   3. On the **Data Migration** page, click **Create Migration Project** in the upper-right corner.

      
   

   

2. On the **Select the source and the destination** page, specify the fields. 

   

   |            Field             |                                                                                                                                                                                                                                                                                                                                                    Description                                                                                                                                                                                                                                                                                                                                                     |
   |------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
   | Migration Project Name       | The name must not exceed 64 characters in length and can contain Chinese characters, digits, and letters.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
   | Tag                          | Click the field and select a tag from the drop-down list. You can also click **Manage Tags** to create, modify, or delete tags. For more information, see [Manage data migration projects by using tags](/en-US/5.user-guide/5.data-migration-1/6.migration-project-management/6.manage-project-tags.md).                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
   |  Source Node | If you have created DB2 LUW data sources, select one from the drop-down list. Only tables with primary keys or unique keys are displayed. Otherwise, click **Add Data Source** in the drop-down list, and add a data source in the dialog box on the right. For more information, see [Add a DB2 LUW data source](/en-US/5.user-guide/3.manage-data-sources/1.add-a-data-source/7.add-db2_luw-data-source.md).  **Notice**  The column specified as the unique key in a DB2 LUW database must have the NOT NULL constraint.                                                                                                                                                                                                       |
   | Destination Node             | If you have created Oracle tenants in OceanBase Database as data sources, select one from the drop-down list. Otherwise, click **Add Data Source** in the drop-down list, and add a data source in the dialog box on the right. For more information, see [Add OceanBase Database physical tables as data source](/en-US/5.user-guide/3.manage-data-sources/1.add-a-data-source/1.add-an-oceanbase-data-source-1/1.add-an-oceanbase-data-source.md).  **Notice**  If the destination data source is not bound to an OCP cluster, the system prompts that the data migration project does not support reverse incremental migration. To enable reverse incremental migration, click **Edit Data Source** next to the prompt, and bind the data source to an OCP cluster. |

   

3. Click **Next** .

   

4. In the dialog box that appears, click **OK** . 

   The project supports only tables with primary keys or non-empty unique indexes. Other tables are automatically filtered.
   

5. On the **Select migration types and objects** page, specify **Migration Type** for the migration project. 

   Migration types include **Schema Migration** , **Full Migration** , **Incremental Migration** , **Full Verification** , and **Reverse Incremental Migration** . If the Oracle tenant of OceanBase Database is not bound to an OCP cluster, you cannot select **Reverse Incremental Migration** . 

   If you select Full Migration or Incremental Migration, we recommend that you select **Full Verification** as well. After the incremental data is synchronized to the destination database, a full verification task is initiated for the migrated data tables in the source and destination databases. 

   **Incremental Migration** involves **DML for Data Change** (`Insert`, `Delete`, and `Update`) and **DDL for Schema Change** . You can select the operations as needed. If you select **Incremental Migration** but do not select all DML operations in the DML for Data Change section, you cannot select **Full Verification** .
   

6. On the **Select migration types and objects** page, select the migration objects. 

   You can select the migration objects in one of the following two ways: **Specified object** and **Matching rules** . If you select **DDL for Schema Change** , only the **Matching rules** option is available. 
   * If you select **Specified object** , select the objects to be migrated on the left, and click **\>** to add them to the list on the right. You can select tables and views of one or more databases for migration. 

     **Notice**

     
     * Chinese characters are not allowed for the name of the table to be migrated and column names.

       
     
     * If the database or table name contains a double dollar sign ($$), you cannot create the migration project.

       
     

     

     When you migrate data from a DB2 LUW database to an Oracle tenant of OceanBase Database, OMS allows you to import objects through text, rename object names, set row filters, view column information, and remove one or all objects to be migrated. 
     

     |      Action       |                                                                                                                                                                                                                                                                                                                                                                                                                             Steps                                                                                                                                                                                                                                                                                                                                                                                                                             |
     |-------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
     | Import Objects    | 1. In the list on the right of the **Data Table** section, click **Import Objects** in the upper-right corner.   2. In the dialog box that appears, click **OK** .  **Notice**  This operation will overwrite previous selections. Proceed with caution.   3. In the **Import Migration Object** dialog box, enter the objects to be migrated, for example, `SCHEMA.TB1 | SCHEMA.TB2 |`.  We recommend that you migrate no more than 10,000 objects at a time.   4. Click **Validate** .   5. After the validation succeeds, click **OK** .                                                                            |
     | Rename            | 1. In the list on the right of the **Data Table** section, move the pointer over the target object.   2. Click **Rename** .   3. Enter a new name and click **OK** .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
     | Settings          | OMS allows you to set `WHERE` conditions to filter data by row and view column information.  1. In the list on the right of the **Data Table** section, move the pointer over the target object.   2. Click **Settings** .   3. In the **Settings** dialog box, enter a `WHERE` clause of a standard SQL statement to configure row filtering.  Only the data meeting the `WHERE` condition is synchronized to the destination data source, thereby filtering data by row. Add escape characters (\`) for keywords of data sources.   4. Click **OK** .  You can also view column information of the migration object in the **View Column** section.    |
     | Remove/Remove All | OMS allows you to remove one or all migration objects.  * Remove a single migration object In the list on the right of the **Data Table** section, move the pointer over the target object, and click **Remove** . The migration object is removed.   * Remove all migration objects In the list on the right of the **Data Table** section, click **Remove All** in the upper-right corner. In the dialog box that appears, click **OK** to remove all migration objects.                                                                                                                                                                                                                                                 |

     
   
   * If you select **Matching rules** , you must configure matching rules. For more information, see [Configure matching rules for migration objects](/en-US/5.user-guide/5.data-migration-1/7.set-a-blacklist-and-a-whitelist.md).

     
   

   

7. Click **Next** . On the **Migration Options** page, specify the fields. 

   

   |     Category      |                                  Field                                   |                                                                                                                                                       Description                                                                                                                                                        |
   |-------------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
   | Basic Settings    | Concurrency for Full Migration                                           | The value can be **Smooth** , **Normal** , or **Fast** . Full data migration tasks with different performance settings consume different resources.  You can also modify the configurations of the specific Checker to customize the concurrency.                                                        |
   | Basic Settings    | Full Verification Concurrency                                            | The value can be **Smooth** , **Normal** , or **Fast** . Different quantities of resources of the source and destination databases are consumed at different concurrencies.  You can also modify the configurations of the specific Checker to customize the concurrency.                                |
   | Basic Settings    | Incremental Record Retention Time                                        | The duration that incremental parsed files are cached in OMS. A longer retention period indicates more disk space occupied by the Store component of OMS.                                                                                                                                                                |
   | Advanced Settings | Whether to Allow Destination Table to Be Not Empty During Full Migration | When a non-empty destination table is allowed for full migration, full verification runs based on the `in` condition and does not need to be removed.                                                                                                                                                                    |
   | Advanced Settings | Whether to Allow Post-indexing                                           | You can specify whether to allow post-indexing after full migration is completed. Post-indexing can shorten the time of full migration.  **Notice**  To enable this feature, select both **Schema Migration** and **Full Migration** on the **Select migration types and objects** page. |

   

8. Click **Precheck** to start precheck on the data migration project. 

   During the precheck, OMS checks whether the database user has the read and write privileges and whether the network connections of the databases meet the requirements. A data migration project can be started only after it passes all check items. If the precheck fails, identify the cause, fix the problem, and run the precheck again until it succeeds. You can modify the system configurations to skip a precheck item.
   

9. Click **Start Task** to start tasks of the project such as schema migration and full migration. 

   If you do not want to immediately start the project, click **Save** to go to the details page of the data migration project. You can start the project later as needed. For more information about project details, see [View details of a data migration project](/en-US/5.user-guide/5.data-migration-1/6.migration-project-management/1.view-migration-task-details.md)
   




Start a data migration project 
---------------------------------------------------

You can start a data migration project only after it passes all check items. If any check item fails, fix the problem manually and perform the check again. After the data migration project starts, run the selected migration types in sequence:

1. Schema migration

   The migration of the definitions of data objects (tables, indexes, constraints, comments, and views) from the source database to the destination OceanBase database. Temporary tables are automatically filtered out. 

   If the source database is not an OceanBase database, OMS performs format conversion and encapsulation based on the syntax definition and standard of the tenant type of the destination OceanBase database, and then replicates the data to the destination OceanBase database. 

   You can view the migration progress of tables and views and perform the following operations on the target object:
   * **View Creation Syntax** : View table creation syntax and modify index creation syntax. 

     Fully compatible DDL syntax executed on the OBServer is displayed. Incompatible syntax is converted before it is displayed.
     
   
   * **Modify the creation syntax and try again** : Check the error information and modify the definition of the conversion result of a failed DDL statement, and then migrate the data to the destination again.

     
   
   * **View Database Return Code** : View the DDL statements executed on the OBServer and the execution error information of a failed schema migration task.

     
   
   * **Retry** / **Retry All Failed Objects** : Retry failed schema migration tasks one by one or retry all failed tasks at a time.

     
   
   * **Skip** / **Batch Skip** : Skip failed schema migration tasks one by one or skip multiple failed tasks at a time. To skip multiple objects at a time, click **Batch Skip** in the upper-right corner. When you skip an object, its index is also skipped.

     
   
   * **Remove** / **Batch Remove** : Remove failed schema migration tasks one by one or remove multiple failed tasks at a time. To remove multiple failed tasks at a time, click **Batch Remove** in the upper-right corner. When you remove an object, its index is also removed.

     
   

   

2. Full migration

   The migration of the existing data from tables in the source database to corresponding tables in the OceanBase database. You can view table objects and table indexes on the **Full Migration** page. The status of the full migration changes to Completed only after migration of the table objects and table indexes is completed. On the **Table Indexes** page, you can click **View Creation Syntax** next to the target table object to view its index creation syntax. 

   You can combine full migration with incremental migration to ensure data consistency between the source and destination databases. If any objects failed to be migrated during a full migration, the causes of the failure are displayed. 

   Full data migration inherits the database and table mapping rules that were configured when the project was created. For more information, see [Data types and syntax conversion](/en-US/5.user-guide/5.data-migration-1/2.data-types-and-syntax-conversion.md).
   

3. Incremental migration

   The migration of changed data of the source database to the corresponding table in the destination OceanBase database. Data changes include data addition, modification, and deletion. 

   When services are continuously writing data to the source database, OMS starts the incremental data pull module to pull incremental data from the source instance, parses and encapsulates the incremental data, and then stores the data in OMS, before it starts the full data migration. 

   After a full data migration task is completed, OMS starts the incremental data replay module to pull incremental data from the incremental data pull module. The incremental data is synchronized to the destination instance after being filtered, mapped, and converted. 

   In the incremental migration section, you can view the performance information of the incremental migration task, such as the migration latency, synchronization timestamp, and migration traffic. 

   When a data migration project is in the **Paused** or **Failed** state, you can modify **Current Timestamp** . When you modify the timestamp for incremental migration, you cannot select a timestamp later than the current time, and the modification may cause data duplication or loss. Proceed with caution.
   

4. Full verification

   After the full migration and incremental migration are completed, OMS automatically initiates a full data verification task to verify the data tables in the source database and the tables in the destination database. 

   OMS also provides corresponding APIs for you to initiate custom data verification tasks in the incremental data synchronization process. 

   You can view the information of specific columns where inconsistent data is detected. OMS runs SQL scripts to correct the data in the destination database based on the data in the source database. The correction operation is not supported if the source database has no corresponding data. 

   OMS allows you to skip full data verification for migration tasks in the **Executing** state. On the **Full Verification** page, click **Skip Full Verification** . In the dialog box that appears, click **Yes** . 

   After the full verification is completed, you can click **Go To Next Stage** to start the forward switchover. After you enter the switchover process, you cannot re-check the current verification task for data comparison and correction.
   

5. Forward switchover

   1. Start forward switchover

      In this step, the migration does not stop. You only confirm the switchover process that is about to start. To start the forward switchover task, click **Start Forward Switchover** . 
      **Notice**

      
      Before you start a forward switchover task, make sure that the writing status of the source data source is either Stopping or Stopped.
      
   
   2. Perform switchover precheck

      Check whether the current project status supports switchover. The precheck involves the following steps:
      * Check the synchronization latency: If the synchronization latency is within 15 seconds after incremental synchronization is started, the switchover passes this check item. If incremental synchronization is not started, the switchover automatically passes this check item.

        
      
      * Check the user write permission on the source side.

        
      

      

      If the switchover passes the precheck, the system automatically performs the next step. If the switchover fails the precheck, the system shows the error details. 

      In this case, you can **Retry** or **Skip** the precheck. 

      After you click **Skip** , you need to click **Skip** again in the dialog box that appears.
      
   
   3. Start the destination Store

      Start incremental data pulling in the destination database. Create and start a destination Store. If the start fails, you can choose to click **Retry** or **Skip** .
      
   
   4. Confirm that writing is stopped in the source database

      In the **Confirm Writing Stopped at Source** section, click **OK** to make sure that no incremental data is generated in the source database.
      
   
   5. Confirm the writing stop timestamp upon synchronization completion

      OMS automatically checks whether the source and destination databases are synchronized to the same timestamp. After the check is completed, the latency and timestamp of the incremental synchronization are displayed.
      
   
   6. Stop forward synchronization

      Stop the JDBCWriter from the source database to the destination database. If the forward synchronization fails to be stopped, you can choose to click **Retry** or **Skip** .
      
   
   7. Process database objects

      In this step, the database objects are migrated, the additional columns and indexes added by OMS are deleted, and the constraints that are automatically ignored during the schema migration are added. You also need to confirm that objects such as triggers and sequences have been manually migrated and that the triggers and foreign keys of the source are disabled. 

      You need to click **Run** to process the database objects. For a running project, you can click **View Logs** or **Skip** . For projects that have been processed, you need to click **Mark as Complete** . After all projects have been marked as completed, proceed to the next step.
      
   
   8. Start reverse incremental migration

      In the **Start Reverse Incremental Migration** section, click **Start Reverse Incremental Migration** to start the JDBCWriter from the destination database to the source database. Wait until the **Successfully started reverse increment** message appears.
      
   

   

6. After reverse incremental migration is started, click the **Reverse Incremental Migration** tab to view detailed information such as **Start Time** and **Reverse Synchronization Performance** . 

   Performance metrics of reverse incremental migration:
   * Latency: The time consumed to synchronize changed incremental data from the destination database to the source database in the unit of seconds.

     
   
   * Migration traffic: The traffic throughput of incremental data synchronization from the destination database to the source database in the unit of MB/s.

     
   

   



