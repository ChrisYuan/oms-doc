Create a project to migrate data from an Oracle tenant of OceanBase Database to a DB2 LUW database 
=======================================================================================================================

OceanBase Migration Service (OMS) allows you to migrate data from an Oracle tenant of OceanBase Database to a DB2 LUW database. OMS supports schema migration, full migration, incremental migration, full verification, and reverse incremental migration. 

Background information 
-------------------------------------------

You can create a data migration project in the OMS console to migrate the existing business data and incremental data from an Oracle tenant of OceanBase Database to a DB2 LUW database through schema migration, full migration, and incremental migration. 

Tables that contain the `pk` and `not null uk` fields have primary keys and do not contain function-based unique keys. If the table in the source Oracle tenant of OceanBase Database contains function-based unique keys in virtual columns or OceanBase Database is earlier than V2.2.77 (excluding V2.2.77), OMS cannot accurately identify whether the table contains function-based unique keys and therefore cannot accurately determine whether the table has a primary key. This slows down full migration and full verification and causes a risk of data inconsistency during incremental synchronization. 

During incremental synchronization, OMS processes data in parallel based on their unique constraints. If a table contains function-based unique keys, the related transaction operations must be performed in sequence during incremental synchronization. Otherwise, data inconsistency may occur. The result of determining whether a table contains function-based unique keys must be delivered by the control layer. If you enable DDL operations for incremental data migration, the `drop index` command is executed on all indexes, which may cause index loss in the destination database. 

When you migrate data from an Oracle tenant of OceanBase Database earlier than V3.2x to a DB2 LUW database, the `row movement` operation causes the following problems:

In a multi-partition table with a globally unique index, if you update the value of the partitioning key of the table, data will be deleted from the old partitions and inserted to the new partitions in the Oracle tenant of OceanBase Database. liboblog cannot determine the dependency of data changes between partitions during migration and may output the INSERT logs first and then the DELETE logs to OMS. OMS may insert new data in the destination database during the replay of the INSERT logs but then delete the inserted data during the replay of the DELETE logs due to the unique index. This results in data loss during the migration process.

Prerequisites 
----------------------------------

* You have created dedicated database users in the source Oracle tenant of OceanBase Database and the destination DB2 LUW database for data migration and granted the corresponding privileges to the users. For more information, see [Create and authorize a database user](/en-US/5.user-guide/5.data-migration-1/4.preparation/1.create-and-authorize-a-database-user.md).

  

* DB2 LUW 10.1, 10.5, 11.1, and 11.5 for Linux and AIX are supported.

  

* Retain logs of OceanBase Database for at least one day in case of unexpected pullback.

  




Data type mappings 
---------------------------------------

### Conversion rules for schema migration 



| Oracle tenant of OceanBase Database |                                                                                                                                                         DB2 LUW database                                                                                                                                                         |
|-------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| CHAR(n CHAR)                        | * Version 10.1: CHAR(n)   * 10.5 and later versions: CHAR(n CODEUNITS32)    **Notice**  Only DB2 LUW 10.5 and later versions support the CODEUNITS32 encoding unit, which contains 0 to 63 characters.                        |
| CHAR(n BYTE)                        | * Version 10.1: CHAR(n)   * 10.5 and later versions: CHAR(n OCTETS)    **Notice**  Only DB2 LUW 10.5 and later versions support the OCTETS encoding unit, which contains 0 to 254 characters.                                 |
| NCHAR(n \<= 2000)                   | NCHAR(n) * If n \> 0 \&\& n \< 255, NCHAR(Math.min(n,63)).   * If n \> 254 \&\& n \< 8169, NVARCHAR(Math.min(n,8168))   * If n \> 8168 \&\& n \< 536870912, NCLOB(Math.min(n,536870911)).    |
| VARCHAR(n CHAR)                     | * Version 10.1: VARCHAR(n)   * 10.5 and later versions: VARCHAR(n CODEUNITS32)    **Notice**  Only DB2 LUW 10.5 and later versions support the CODEUNITS32 encoding unit, which contains 0 to 63 characters.                  |
| VARCHAR(n BYTE)                     | * Version 10.1: VARCHAR(n)   * 10.5 and later versions: VARCHAR(n OCTETS)    **Notice**  Only DB2 LUW 10.5 and later versions support the OCTETS encoding unit, which contains 0 to 254 characters.                           |
| VARCHAR2(n CHAR)                    | * Version 10.1: VARCHAR(n)   * 10.5 and later versions: VARCHAR(n CODEUNITS32)    **Notice**  Only DB2 LUW 10.5 and later versions support the CODEUNITS32 encoding unit, which contains 0 to 63 characters.                  |
| VARCHAR2(n BYTE)                    | * Version 10.1: VARCHAR(n)   * 10.5 and later versions: VARCHAR(n OCTETS)    **Notice**  Only DB2 LUW 10.5 and later versions support the OCTETS encoding unit, which contains 0 to 254 characters.                           |
| NVARCHAR2(n)                        | NVARCHAR(n) * If n \> 0 \&\& n \< 8169, NVARCHAR(Math.min(n,8168)).   * If n \> 8168 \&\& n \< 536870912, NCLOB(n).                                                                                                           |
| NUMBER(p,s)                         | NUMERIC(p,s)                                                                                                                                                                                                                                                                                                                     |
| RAW(n)                              | VARCHAR(n) FOR BIT DATA                                                                                                                                                                                                                                                                                                          |
| CLOB                                | CLOB(n)                                                                                                                                                                                                                                                                                                                          |
| NCLOB                               | NCLOB(n)                                                                                                                                                                                                                                                                                                                         |
| BLOB                                | BLOB(n)                                                                                                                                                                                                                                                                                                                          |
| FLOAT(p) {1 \<= p \<= 63}           | FLOAT                                                                                                                                                                                                                                                                                                                            |
| FLOAT(p) {64 \<= p \<= 126}         | DOUBLE                                                                                                                                                                                                                                                                                                                           |
| BINARY_FLOAT                        | FLOAT                                                                                                                                                                                                                                                                                                                            |
| BINARY_DOUBLE                       | DOUBLE                                                                                                                                                                                                                                                                                                                           |
| DATE (Includes time part)           | TIMESTAMP(fsp)                                                                                                                                                                                                                                                                                                                   |
| DATE                                | TIMESTAMP(0)                                                                                                                                                                                                                                                                                                                     |
| TIMESTAMP                           | TIMESTAMP                                                                                                                                                                                                                                                                                                                        |
| TIMESTAMP WITH TIME ZONE            | TIMESTAMP(fsp) \[Microseconds\]                                                                                                                                                                                                                                                                                                  |
| TIMESTAMP WITH LOCAL TIME ZONE      | TIMESTAMP(fsp) \[Microseconds\]                                                                                                                                                                                                                                                                                                  |
| INTERVAL YEAR TO MONTH              | **Warning**  This type may incur conversion risks. Modify it manually.                                                                                                                                                                                                                                           |
| INTERVAL DAY (dp) TO SECOND(sp)     | **Warning**  This type may incur conversion risks. Modify it manually.                                                                                                                                                                                                                                           |
| ROWID ( \>= 2.2.70)                 | CHAR(18)                                                                                                                                                                                                                                                                                                                         |
| UROWID(n) ( \>= 2.2.70)             | VARCHAR(n)                                                                                                                                                                                                                                                                                                                       |


**Notice**



* The CHAR and VARCHAR2 data types in an Oracle tenant of OceanBase Database can store multi-byte encoded data. Therefore, if single-byte encoding units are used in reverse conversion, the data types may be not long enough.

  

* In a DB2 LUW database, the lengths of data types as well as the OCTETS, CODEUNITS16, and CODEUNITS32 encoding units must be considered for data storage. 

  Only DB2 LUW 10.5 and later versions support the OCTETS and CODEUNITS32 encoding units.
  




### Limits 

* OMS cannot migrate a table without a primary key from an Oracle tenant of OceanBase Database to a DB2 LUW database.

  

* The maximum timestamp precision of a DB2 LUW database is 12, while that of an Oracle tenant of OceanBase Database is 9. Therefore, the data is truncated when it is migrated from a DB2 LUW database to an Oracle tenant of OceanBase Database.

  

* Length limits

  * The data of the CHAR or BINARY type cannot exceed 255 bytes in length in a DB2 LUW database. If the data written to an Oracle tenant of OceanBase Database exceeds 255 bytes in length, an error will occur during migration.

    
  
  * The data of the VARCHAR or BINARY type cannot exceed 32,673 bytes in length in a DB2 LUW database. If the data written to an Oracle tenant of OceanBase Database exceeds 32,673 bytes in length, an error will occur during migration.

    
  
  * The Decimal(dp, ds) in a DB2 LUW database is equivalent to the NUMBER of an Oracle tenant of OceanBase Database. The length of dp cannot exceed 31 and must be greater than that of ds. 

    The number written to an Oracle tenant of OceanBase Database cannot exceed the maximum allowed number. By default, the data of the NUMBER, INT, SMALLINT, and NUMBER(\*, s) types is 38 bytes in length in an Oracle tenant of OceanBase Database. You need to explicitly define the NUMBER(p,s) to the length that is compatible with the business application and the source and destination databases.
    
  
  * The length of data of the NCHAR type cannot exceed 63 bytes in a DB2 LUW database or 2,000 bytes in an Oracle tenant of OceanBase Database. Therefore, if a field where data of the NCHAR type exceeds 63 bytes in an Oracle tenant of OceanBase Database, the field is truncated when it is migrated to a DB2 LUW database. This causes data loss.

    
  

  

* Data type limits

  * The data of the TIME type in a DB2 LUW database cannot be migrated as the partitioning key.

    
  
  * The data of the XML type is not supported.

    
  
  * We do not recommend that you use CODEUNITS16 or CODEUNITS32 to define or store multi-byte data of the NCHAR, GRAPHIC, or other types.

    
  
  * The default value of the BLOB-type data cannot be modified.

    
  
  * When you migrate data from OceanBase Database V1.4.x, OMS does not support primary keys that contain data of the FLOAT or DOUBLE type.

    
  
  * Spaces on the right side of data of the VARCHAR and VARGRAPHIC type are not compared in a DB2 LUW database. If the primary key lacks a space during incremental data migration, the data can become inconsistent. This causes data loss in the destination database.

    
  
  * Note that when a data type in a DB2 LUW database is converted to the LOB type in an Oracle tenant of OceanBase Database, the data of the LOB type cannot exceed 48 MB in size.

    
  

  




Dynamic DDL operations 
-------------------------------------------

### Supported DDL operations 

* `create table`

  

* `alter table`

  

* `drop table`

  

* `truncate table`

  

* `rename table`

  

* `create index`

  

* `drop index`

  

* Operations for adding and removing comments of tables or columns

  




### Limits 

* Create Table

  

  |          Category           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
  |-----------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
  | Syntax restriction          | * The Create like syntax is not supported.   * Syntax that contains column type aliases (non-native data types) is not supported.   * Syntax that contains the Select or Subquery statement is not supported.   * An error may be returned if the generated column contains incompatible functions or complex expressions.   * Syntax that contains the XML data type is not supported.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
  | Column attribute processing | * Column names, column types, null flag bits, and default values are processed. Other column attribute values are ignored.   * DDL statements do not support attributes of generated columns and do not process the definitions of generated columns.   * Default values with functions should be used with caution. If the functions do not exist in the destination database, the table fails to be created.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
  | Index processing            | * We recommend that you explicitly define indexes, including primary keys, unique keys, and CHECK constraints. CHECK constraints are not preferred. Anonymous indexes automatically generate names, which may cause name conflicts.   * Functions should be used with caution. If the functions do not exist in the destination database, the table fails to be created.   * Foreign keys are not recommended. OMS does not support synchronization of tables with foreign keys. Foreign keys will be discarded during DDL conversion.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
  | Partition                   | * OMS supports synchronization of time-based range partitions but directly translates expressions for non-time-based range partitions. We do not recommend that you use functions in partitions. Functions may cause incompatibility of syntax and further cause failure of the migration project.   * The range of a partition cannot start or end with a representation related to `MINVALUE` or `MAXVALUE`.   * Partitions of the DB2 LUW database must contain the `ending` field with the `exclusive` attribute. Otherwise, an error is returned.   * The DB2 LUW database does not support the `every` syntax extension for partitions to prevent calculation errors .   * In the DB2 LUW database, a partition column is added to the primary key of a table. If the primary key and partitioning key have no intersection, the primary key is changed to a unique index to adapt to the partition rules and constraints of OceanBase Database.   * In the DB2 LUW database, the `create/alter table add partition` statement must contain a partition name. Anonymous partitions are not supported.    |

  

* Alter Table

  * The `alter table add primary` statement in a DB2 LUW database is converted to the `add unique` statement in an Oracle tenant of OceanBase Database. OceanBase Database does not support adding a primary key after a table is created. Therefore, you must explicitly specify the primary key when you create a table.

    
  
  * When you execute the `alter table` statement to modify data of the LOB type, you must specify the data length. However, you cannot reduce the length of the LOB-type data by executing the `alter table` statement.

    
  
  * The `unique index` cannot be empty in a DB2 LUW database. Therefore, the column that is specified as the unique key requires NOT NULL constraint. In other words, when you execute the `alter table add unique` statement, you must specify the NOT NULL constraint for the related column.

    
  

  

* Create Index

  You cannot specify Local in the statement to create a regular table in OceanBase Database.
  

* Parse DDL statements

  When you parse DDL statements, you can refer to the official DB2 database documentation to ensure the maximum compatibility. 

  Note the following limits when you parse the source DDL statements in a DB2 LUW database:
  * The DB2 LUW database checks the schema to refresh the cached, parsed source DDL statements. We recommend that you only parse one type of DDL statements at a time and proceed after you confirm the results.

    
  
  * Avoid frequent `create` and `drop` operations on partitioned tables. We recommend that you execute the `create partition table` statement, confirm that the table is synchronized, and then execute the `drop partition table` statement.

    
  
  * The DB2 LUW database can only parse an object whose name consists of letters, underscores (_), and digits, begins with a letter or underscore, and does not contain a DB2 LUW keyword.

    
  
  * If you do not specify the **DDL for Schema Change** field when you create the project, you must comply with the following rules to add or delete columns in the source database:

    * Add a column in the destination database first and then in the source database.

      
    
    * Delete a column in the destination database first and then in the source database.

      
    

    
  

  




Create a data migration project 
----------------------------------------------------

1. Create a data migration link. 

   1. Log on to the OMS console.

      
   
   2. In the left-side navigation pane, click **Data Migration** .

      
   
   3. On the **Data Migration** page, click **Create Migration Project** in the upper-right corner.

      
   

   

2. On the **Select the source and the destination** page, specify the fields. 

   

   |         Field          |                                                                                                                                                                                                                                                                                                                                          Description                                                                                                                                                                                                                                                                                                                                          |
   |------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
   | Migration Project Name | The name must not exceed 64 characters in length and can contain Chinese characters, digits, and letters.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
   | Tag                    | Click the field and select a tag from the drop-down list. You can also click **Manage Tags** to create, modify, or delete tags. For more information, see [Manage data migration projects by using tags](/en-US/5.user-guide/5.data-migration-1/6.migration-project-management/6.manage-project-tags.md).                                                                                                                                                                                                                                                                                                                                                                                                                                        |
   | Source Node            | If you have created Oracle tenants in OceanBase Database as data sources, select one from the drop-down list. Otherwise, click **Add Data Source** in the drop-down list, and add a data source in the dialog box on the right. For more information, see [Add OceanBase Database physical tables as data source](/en-US/5.user-guide/3.manage-data-sources/1.add-a-data-source/1.add-an-oceanbase-data-source-1/1.add-an-oceanbase-data-source.md).  **Notice**  If the source data source is not bound to an OCP cluster, the system prompts that the data migration project does not support incremental migration. To enable incremental migration, click **Edit Data Source** next to the prompt, and bind the data source to an OCP cluster. |
   | Destination Node       | The DB2 LUW data source that you created. Only tables with primary keys or unique keys are displayed.  If you have created DB2 LUW data sources, select one from the drop-down list. Only tables with primary keys or unique keys are displayed. Otherwise, click **Add Data Source** in the drop-down list, and add a data source in the dialog box on the right. For more information, see [Add a DB2 LUW data source](/en-US/5.user-guide/3.manage-data-sources/1.add-a-data-source/7.add-db2_luw-data-source.md).  **Notice**  The column specified as the unique key in a DB2 LUW database must have the NOT NULL constraint.                                                           |

   

3. Click **Next** .

   

4. In the dialog box that appears, click **OK** . 

   The project supports only tables with primary keys or non-empty unique indexes. Other tables are automatically filtered.
   

5. On the **Select migration types and objects** page, specify **Migration Type** for the migration project. 

   Migration types include **Schema Migration** , **Full Migration** , **Incremental Migration** , **Full Verification** , and **Reverse Incremental Migration** . If the Oracle tenant of OceanBase Database is not bound to an OCP cluster, you cannot select **Incremental Migration** . 

   If you select Full Migration or Incremental Migration, we recommend that you select **Full Verification** as well. After the incremental data is synchronized to the destination database, a full verification task is initiated for the migrated data tables in the source and destination databases. 

   **Incremental Migration** involves **DML for Data Change** (`Insert`, `Delete`, and `Update`) and **DDL for Schema Change** . You can select the operations as needed. If you select **Incremental Migration** but do not select all DML operations in the DML for Data Change section, you cannot select **Full Verification** . 

   During the incremental migration from an Oracle tenant of OceanBase Database to a DB2 LUW database, the comments are automatically filtered.
   

6. On the **Select migration types and objects** page, select the migration objects. 

   You can select the migration objects in one of the following two ways: **Specified object** and **Matching rules** . If you select **DDL for Schema Change** , only the **Matching rules** option is available. 
   * If you select **Specified object** , select the objects to be migrated on the left, and click **\>** to add them to the list on the right. You can select tables and views of one or more databases for migration. 

     **Notice**

     
     * Chinese characters are not allowed for the name of the table to be migrated and column names.

       
     
     * If the database or table name contains a double dollar sign ($$), you cannot create the migration project.

       
     

     

     When you migrate data from an Oracle tenant of OceanBase Database to a DB2 LUW database, OMS allows you to import objects by using text files, rename object names, set row filters, view column information, and remove one or all objects to be migrated. 
     

     |      Action       |                                                                                                                                                                                                                                                                                                                                                                                                                             Steps                                                                                                                                                                                                                                                                                                                                                                                                                             |
     |-------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
     | Import Objects    | 1. In the list on the right of the **Data Table** section, click **Import Objects** in the upper-right corner.   2. In the dialog box that appears, click **OK** .  **Notice**  This operation will overwrite previous selections. Proceed with caution.   3. In the **Import Migration Object** dialog box, enter the objects to be migrated, for example, `SCHEMA.TB1 | SCHEMA.TB2 |`.  We recommend that you migrate no more than 10,000 objects at a time.   4. Click **Validate** .   5. After the validation succeeds, click **OK** .                                                                            |
     | Rename            | 1. In the list on the right of the **Data Table** section, move the pointer over the target object.   2. Click **Rename** .   3. Enter a new name and click **OK** .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
     | Settings          | OMS allows you to set `WHERE` conditions to filter data by row and view column information.  1. In the list on the right of the **Data Table** section, move the pointer over the target object.   2. Click **Settings** .   3. In the **Settings** dialog box, enter a `WHERE` clause of a standard SQL statement to configure row filtering.  Only the data meeting the `WHERE` condition is synchronized to the destination data source, thereby filtering data by row. Add escape characters (\`) for keywords of data sources.   4. Click **OK** .  You can also view column information of the migration object in the **View Column** section.    |
     | Remove/Remove All | OMS allows you to remove one or all migration objects.  * Remove a single migration object In the list on the right of the **Data Table** section, move the pointer over the target object, and click **Remove** . The migration object is removed.   * Remove all migration objects In the list on the right of the **Data Table** section, click **Remove All** in the upper-right corner. In the dialog box that appears, click **OK** to remove all migration objects.                                                                                                                                                                                                                                                 |

     
   
   * If you select **Matching rules** , you must configure matching rules. For more information, see [Configure matching rules for migration objects](/en-US/5.user-guide/5.data-migration-1/7.set-a-blacklist-and-a-whitelist.md).

     
   

   

7. Click **Next** . On the **Migration Options** page, specify the fields. 

   

   |     Category      |                                  Field                                   |                                                                                                                                                       Description                                                                                                                                                        |
   |-------------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
   | Basic Settings    | Concurrency for Full Migration                                           | The value can be **Smooth** , **Normal** , or **Fast** . Full data migration tasks with different performance settings consume different resources.  You can also modify the configurations of the specific Checker to customize the concurrency.                                                        |
   | Basic Settings    | Full Verification Concurrency                                            | The value can be **Smooth** , **Normal** , or **Fast** . Different quantities of resources of the source and destination databases are consumed at different concurrencies.  You can also modify the configurations of the specific Checker to customize the concurrency.                                |
   | Basic Settings    | Incremental Record Retention Time                                        | The duration that incremental parsed files are cached in OMS. A longer retention period indicates more disk space occupied by the Store component of OMS.                                                                                                                                                                |
   | Advanced Settings | Whether to Allow Destination Table to Be Not Empty During Full Migration | When a non-empty destination table is allowed for full migration, full verification runs based on the `in` condition and does not need to be removed.                                                                                                                                                                    |
   | Advanced Settings | Whether to Allow Post-indexing                                           | You can specify whether to allow post-indexing after full migration is completed. Post-indexing can shorten the time of full migration.  **Notice**  To enable this feature, select both **Schema Migration** and **Full Migration** on the **Select migration types and objects** page. |

   

8. Click **Precheck** to start precheck on the data migration project. 

   During the precheck, OMS checks whether the database user has the read and write privileges and whether the network connections of the databases meet the requirements. A data migration project can be started only after it passes all check items. If the precheck fails, identify the cause, fix the problem, and run the precheck again until it succeeds. You can modify the system configurations to skip a precheck item.
   

9. Click **Start Task** to start tasks of the project such as schema migration and full migration. 

   If you do not want to immediately start the project, click **Save** to go to the details page of the data migration project. You can start the project later as needed. For more information about project details, see [View details of a data migration project](/en-US/5.user-guide/5.data-migration-1/6.migration-project-management/1.view-migration-task-details.md).
   




Start a data migration project 
---------------------------------------------------

You can start a data migration project only after it passes all check items. For any failed check items, fix the problem manually and check again. After the data migration project starts, run the selected migration types in sequence: 

1. Schema migration

   You can view the migration progress of tables and views and perform the following operations on the target object:
   * **View Creation Syntax** : View table creation syntax and modify index creation syntax. 

     Fully compatible DDL syntax executed on the OBServer is displayed. Incompatible syntax is converted before it is displayed.
     
   
   * **Modify the creation syntax and try again** : Check the error information and modify the definition of the conversion result of a failed DDL statement, and then migrate the data to the destination again.

     
   
   * **View Database Return Code** : View the DDL statements executed on the OBServer and the execution error information of a failed schema migration task.

     
   
   * **Retry** / **Retry All Failed Objects** : Retry failed schema migration tasks one by one or retry all failed tasks at a time.

     
   
   * **Skip** / **Batch Skip** : Skip failed schema migration tasks one by one or skip multiple failed tasks at a time. To skip multiple failed tasks at a time, click **Batch Skip** in the upper-right corner. When you skip an object, its index is also skipped.

     
   
   * **Remove** / **Batch Remove** : Remove failed schema migration tasks one by one or remove multiple failed tasks at a time. To remove multiple failed tasks at a time, click **Batch Remove** in the upper-right corner. When you remove an object, its index is also removed.

     
   

   

2. Full migration

   The migration of the existing data from tables in the source database to corresponding tables in the OceanBase database. You can view table objects and table indexes on the **Full Migration** page. The status of the full migration changes to Completed only after migration of the table objects and table indexes is completed. On the **Table Indexes** page, you can click **View Creation Syntax** next to the target table object to view its index creation syntax. 

   You can combine full migration with incremental migration to ensure data consistency between the source and destination databases. If any objects failed to be migrated during a full migration, the causes of the failure are displayed. 

   Full data migration inherits the database and table mapping rules that were configured when the project was created. For more information, see [Data types and syntax conversion](/en-US/5.user-guide/5.data-migration-1/2.data-types-and-syntax-conversion.md).
   

3. Incremental migration

   The migration of changed data of the source database to the corresponding table in OceanBase Database. Data changes include data addition, modification, and deletion. 

   When services are continuously writing data to the source database, OMS starts the incremental data pull module to pull incremental data from the source instance, parses and encapsulates the incremental data, and then stores the data in OMS, before it starts the full data migration. 

   After a full data migration task is completed, OMS starts the incremental data replay module to pull incremental data from the incremental data pull module. The incremental data is synchronized to the destination instance after being filtered, mapped, and converted. 

   In the incremental migration section, you can view the performance information of the incremental migration task, such as the migration latency, synchronization timestamp, and migration traffic. 

   When a data migration project is in the **Paused** or **Failed** state, you can modify **Current Timestamp** . When you modify the timestamp for incremental migration, you cannot select a timestamp later than the current time, and the modification may cause data duplication or loss. Proceed with caution.
   

4. Full verification

   After the full data migration and incremental data migration are completed, OMS automatically initiates a full data verification task to verify the data tables in the source database and the tables in the destination database. 

   OMS also provides corresponding APIs for you to initiate custom data verification tasks in the incremental data synchronization process. 

   You can view the information of specific columns where inconsistent data is detected. OMS runs SQL scripts to correct the data in the destination database based on the data in the source database. The correction operation is not supported if the source database has no corresponding data. 

   OMS allows you to skip full verification for migration tasks in the **Executing** state. On the **Full Verification** page, click **Skip Full Verification** . In the dialog box that appears, click **Yes** . 

   After full verification is completed, click **Go To Next Stage** . In the dialog box that appears, click **OK** . 
   **Notice**

   

   After you enter the switchover process, you cannot re-check the current verification task for data comparison and correction.
   

5. Forward switchover

   1. Start forward switchover

      In this step, the migration does not stop. You only confirm the switchover process that is about to start. To start the forward switchover task, click **Start Forward Switchover** . 
      **Notice**

      
      Before you start a forward switchover task, make sure that the writing status of the source data source is either Stopping or Stopped.
      
   
   2. Perform switchover precheck

      Check whether the current project status supports switchover. The precheck involves the following steps:
      * Check the synchronization latency: If the synchronization latency is within 15 seconds after incremental synchronization is started, the switchover passes this check item. If incremental synchronization is not started, the switchover automatically passes this check item.

        
      
      * Check the user write privilege on the source side.

        
      

      

      If the switchover passes the precheck, the system automatically performs the next step. If the switchover fails the precheck, the system shows the error details. 

      In this case, you can **Retry/Skip** the precheck. 

      After you click **Skip** , you need to click **Skip** again in the dialog box that appears.
      
   
   3. Start the destination Store

      Start incremental data pulling in the destination database. Create and start a destination Store. If the start fails, you can choose to click **Retry** or **Skip** .
      
   
   4. Confirm that writing is stopped in the source database

      In the **Confirm writing stopped on the source side** section, click **OK** . You need to ensure that no incremental data is generated from the source database.
      
   
   5. Confirm the writing stop timestamp upon synchronization completion

      OMS automatically checks whether the source and destination databases are synchronized to the same timestamp. After the check is completed, the latency and timestamp of the incremental synchronization are displayed.
      
   
   6. Stop forward synchronization

      Stop the JDBCWriter from the source database to the destination database. If the forward synchronization fails to be stopped, you can choose to click **Retry** or **Skip** .
      
   
   7. Process database objects

      In this step, database objects are migrated, additional objects are deleted, and database objects that are not migrated are added. 

      You need to click **Run** to execute the database objects. For a running project, you can click **View Logs** or **Skip** . For projects that have been processed, you need to click **Mark as Complete** . After all projects have been marked as completed, proceed to the next step.
      
   
   8. Start reverse incremental migration

      In the **Start Reverse Incremental Migration** section, click **Start Reverse Incremental Migration** to start the JDBCWriter from the destination database to the source database. Wait until the **Successfully started reverse increment** message appears.
      
   

   

6. After reverse incremental migration is started, click the **Reverse Incremental Migration** tab to view detailed information such as **Start Time** and **Reverse Synchronization Performance** . 

   Performance metrics of reverse incremental migration:
   * Latency: The time consumed to synchronize changed incremental data from the destination database to the source database in the unit of seconds.

     
   
   * Migration traffic: The traffic throughput of incremental data synchronization from the destination database to the source database in the unit of MB/s.

     
   

   



